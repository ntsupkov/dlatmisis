{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Домашнее задание: Visual Question Answering (VQA)\n",
        "\n",
        "## 1. Введение\n",
        "\n",
        "### Что такое VQA?\n",
        "\n",
        "**Visual Question Answering (VQA)** — задача, в которой модель должна ответить на текстовый вопрос об изображении. Например:\n",
        "- Изображение: фотография кота\n",
        "- Вопрос: \"Какого цвета кот?\"\n",
        "- Ответ: \"Рыжий\"\n",
        "\n",
        "Это мультимодальная задача, требующая понимания как визуальной, так и текстовой информации\n",
        "\n",
        "### Зачем нужны мультимодальные модели?\n",
        "\n",
        "Традиционные модели работают либо с изображениями, либо с текстом. Мультимодальные модели объединяют оба типа данных:\n",
        "- **Простой подход:** объединение эмбеддингов из разных моделей (ResNet + T5)\n",
        "- **Продвинутый подход:** сквозное обучение (CLIP, LLaVA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 2. Подготовка окружения\n",
        "\n",
        "Установим необходимые библиотеки для работы с моделями и интерфейсами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision transformers open_clip_torch gradio pillow pandas accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from transformers import T5EncoderModel, T5Tokenizer, CLIPProcessor, CLIPModel\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "import open_clip\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Используемое устройство: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Память: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data"
      },
      "source": [
        "## 3. Подготовка данных\n",
        "\n",
        "Создадим небольшой датасет для тестирования. Для простоты возьмем несколько изображений из CIFAR-10 и составим вопросы вручную\n",
        "\n",
        "### Задание 3.1: Загрузите датасет CIFAR-10\n",
        "\n",
        "**Что нужно сделать:**\n",
        "- Загрузите тестовую часть CIFAR-10 (используйте `torchvision.datasets.CIFAR10`)\n",
        "- Выберите 5-7 изображений из разных классов\n",
        "- Сохраните их в список `sample_images`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_cifar"
      },
      "outputs": [],
      "source": [
        "cifar_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "sample_images = []\n",
        "sample_labels = []\n",
        "\n",
        "cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
        "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "selected_classes = set()\n",
        "for img, label in cifar_dataset:\n",
        "    if label not in selected_classes:\n",
        "        sample_images.append(img)\n",
        "        sample_labels.append(label)\n",
        "        selected_classes.add(label)\n",
        "    if len(sample_images) == 7:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataset"
      },
      "source": [
        "### Задание 3.2: Создайте DataFrame с вопросами и ответами\n",
        "\n",
        "**Что нужно сделать:**\n",
        "- Для каждого изображения придумайте 1-2 вопроса\n",
        "- Вопросы могут быть о: цвете, типе объекта, количестве объектов, действиях\n",
        "- Создайте pandas DataFrame с колонками: `image_id`, `question`, `answer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_qa"
      },
      "outputs": [],
      "source": [
        "qa_data = {\n",
        "    'image_id': [],\n",
        "    'question': [],\n",
        "    'answer': []\n",
        "}\n",
        "\n",
        "questions_answers = [\n",
        "    [(0, 'What object is in the image?', cifar_classes[sample_labels[0]]),\n",
        "     (0, 'Is this a vehicle?', 'yes' if sample_labels[0] in [0, 1, 8, 9] else 'no')],\n",
        "    [(1, 'What object is in the image?', cifar_classes[sample_labels[1]]),\n",
        "     (1, 'Is this an animal?', 'yes' if sample_labels[1] in [2, 3, 4, 5, 6, 7] else 'no')],\n",
        "    [(2, 'What object is in the image?', cifar_classes[sample_labels[2]]),\n",
        "     (2, 'Can this fly?', 'yes' if sample_labels[2] in [0, 2] else 'no')],\n",
        "    [(3, 'What object is in the image?', cifar_classes[sample_labels[3]]),\n",
        "     (3, 'Is this a living creature?', 'yes' if sample_labels[3] in [2, 3, 4, 5, 6, 7] else 'no')],\n",
        "    [(4, 'What object is in the image?', cifar_classes[sample_labels[4]]),\n",
        "     (4, 'Is this used for transportation?', 'yes' if sample_labels[4] in [0, 1, 8, 9] else 'no')],\n",
        "    [(5, 'What object is in the image?', cifar_classes[sample_labels[5]]),\n",
        "     (5, 'Does this have legs?', 'yes' if sample_labels[5] in [3, 4, 5, 6, 7] else 'no')],\n",
        "    [(6, 'What object is in the image?', cifar_classes[sample_labels[6]]),\n",
        "     (6, 'Is this found in water?', 'yes' if sample_labels[6] in [6, 8] else 'no')]\n",
        "]\n",
        "\n",
        "for qa_list in questions_answers:\n",
        "    for img_id, q, a in qa_list:\n",
        "        qa_data['image_id'].append(img_id)\n",
        "        qa_data['question'].append(q)\n",
        "        qa_data['answer'].append(a)\n",
        "\n",
        "df = pd.DataFrame(qa_data)\n",
        "print(f\"\\n Создан датасет: {len(df)} вопросов для {len(sample_images)} изображений\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize_data"
      },
      "outputs": [],
      "source": [
        "def visualize_samples(images, df, n_samples=3):\n",
        "    fig, axes = plt.subplots(1, min(n_samples, len(images)), figsize=(15, 5))\n",
        "    if n_samples == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for idx, ax in enumerate(axes):\n",
        "        if idx < len(images):\n",
        "            ax.imshow(images[idx])\n",
        "            ax.axis('off')\n",
        "            questions = df[df['image_id'] == idx]\n",
        "            title = f\"Image {idx}\\n\"\n",
        "            for _, row in questions.iterrows():\n",
        "                title += f\"Q: {row['question'][:30]}...\\n\"\n",
        "            ax.set_title(title, fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_samples(sample_images, df, n_samples=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baseline"
      },
      "source": [
        "## 4. Baseline: ResNet + T5\n",
        "\n",
        "Создадим простой бейз, который:\n",
        "1. Извлекает эмбеддинги изображений через предобученный ResNet50\n",
        "2. Извлекает эмбеддинги вопросов через T5-small\n",
        "3. Объединяет их и предсказывает ответ через MLP\n",
        "\n",
        "### Задание 4.1: Извлеките эмбеддинги изображений\n",
        "\n",
        "**Что нужно сделать:**\n",
        "- Загрузите предобученный ResNet50\n",
        "- Удалите последний слой классификации (голову)\n",
        "- Извлеките эмбеддинги для всех изображений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "resnet_embeddings"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder:\n",
        "    def __init__(self):\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
        "        self.model.eval()\n",
        "        self.model.to(device)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    \n",
        "    def encode(self, images):\n",
        "        batch = torch.stack([self.transform(img) for img in images]).to(device)\n",
        "        with torch.no_grad():\n",
        "            embeddings = self.model(batch)\n",
        "        return embeddings.squeeze()\n",
        "\n",
        "image_encoder = ImageEncoder()\n",
        "image_embeddings = image_encoder.encode(sample_images)\n",
        "\n",
        "print(f\"Размерность эмбеддингов изображений: {image_embeddings.shape if image_embeddings is not None else 'Fuck'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5_embeddings"
      },
      "source": [
        "### Задание 4.2: Извлеките эмбеддинги вопросов\n",
        "\n",
        "**Что нужно сделать:**\n",
        "- Загрузите T5-small encoder и tokenizer\n",
        "- Токенизируйте все вопросы\n",
        "- Получите эмбеддинги (используйте mean pooling по последней скрытой размерности)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5_encoder"
      },
      "outputs": [],
      "source": [
        "class TextEncoder:\n",
        "    def __init__(self, model_name='t5-small'):\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.model = T5EncoderModel.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "        self.model.to(device)\n",
        "    \n",
        "    def encode(self, texts):\n",
        "        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        return embeddings\n",
        "\n",
        "text_encoder = TextEncoder()\n",
        "question_embeddings = text_encoder.encode(df['question'].tolist())\n",
        "print(f'Размерность эмбеддингов вопросов: {question_embeddings.shape if question_embeddings is not None else \"\"}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlp_classifier"
      },
      "source": [
        "### Задание 4.3: Обучите MLP-классификатор\n",
        "\n",
        "**Что нужно сделать:**\n",
        "- Объедините эмбеддинги изображений и вопросов (конкатенация)\n",
        "- Создайте словарь всех уникальных ответов\n",
        "- Реализуйте простой MLP (2-3 слоя)\n",
        "- Обучите модель на нескольких эпохах\n",
        "\n",
        "**Примечание:** Из-за маленького датасета не ожидайте высокую точность. Цель — понять архитектуру."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlp_model"
      },
      "outputs": [],
      "source": [
        "class VQAClassifier(nn.Module):\n",
        "    def __init__(self, image_dim, text_dim, num_classes, hidden_dim=512):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(image_dim + text_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.fc3 = nn.Linear(hidden_dim // 2, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "    \n",
        "    def forward(self, image_emb, text_emb):\n",
        "        x = torch.cat([image_emb, text_emb], dim=1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "answer_vocab = {ans: idx for idx, ans in enumerate(df['answer'].unique())}\n",
        "idx_to_answer = {idx: ans for ans, idx in answer_vocab.items()}\n",
        "print(f\"\\nСловарь ответов ({len(answer_vocab)} классов): {list(answer_vocab.keys())}\")\n",
        "\n",
        "model = VQAClassifier(image_embeddings.shape[1], question_embeddings.shape[1], len(answer_vocab))\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_mlp"
      },
      "outputs": [],
      "source": [
        "img_emb_per_question = torch.stack([image_embeddings[img_id] for img_id in df['image_id']]).to(device)\n",
        "labels = torch.tensor([answer_vocab[ans] for ans in df['answer']]).to(device)\n",
        "\n",
        "num_epochs = 100\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(img_emb_per_question, question_embeddings)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "model.eval()\n",
        "print('\\nОбучение завершено')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baseline_inference"
      },
      "source": [
        "### Задание 4.4: Протестируйте baseline\n",
        "\n",
        "**Что нужно сделать:**\n",
        "- Выберите 2-3 примера из датасета\n",
        "- Получите предсказания модели\n",
        "- Выведите изображение, вопрос, истинный и предсказанный ответ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_baseline"
      },
      "outputs": [],
      "source": [
        "def predict_baseline(image_id, question):\n",
        "    img_emb = image_embeddings[image_id].unsqueeze(0).to(device)\n",
        "    q_emb = text_encoder.encode([question])\n",
        "    with torch.no_grad():\n",
        "        output = model(img_emb, q_emb)\n",
        "        pred_idx = torch.argmax(output, dim=1).item()\n",
        "    return idx_to_answer[pred_idx]\n",
        "\n",
        "for i in range(min(3, len(df))):\n",
        "    row = df.iloc[i]\n",
        "    pred = predict_baseline(row['image_id'], row['question'])\n",
        "    print(f\"\\nПример {i+1}:\")\n",
        "    print(f\"Image ID: {row['image_id']}\")\n",
        "    print(f\"Question: {row['question']}\")\n",
        "    print(f\"True Answer: {row['answer']}\")\n",
        "    print(f\"Predicted: {pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clip"
      },
      "source": [
        "## 5. CLIP Zero-Shot Baseline\n",
        "\n",
        "CLIP — это мультимодальная модель, обученная связывать изображения и тексты. Мы используем её для zero-shot VQA:\n",
        "1. Для каждой пары (изображение, вопрос) сформируем набор возможных ответов\n",
        "2. Составим промпты типа \"A photo of {answer}\"\n",
        "3. CLIP выберет наиболее вероятный ответ\n",
        "\n",
        "### Задание 5.1: Загрузите CLIP\n",
        "\n",
        "**Что нужно сделать:**\n",
        "- Загрузите CLIP модель (используйте `openai/clip-vit-base-patch32` через transformers)\n",
        "- Или используйте `open_clip` библиотеку"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_clip"
      },
      "outputs": [],
      "source": [
        "clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
        "clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "clip_model.to(device)\n",
        "clip_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clip_inference"
      },
      "source": [
        "### Задание 5.2: Реализуйте zero-shot VQA с CLIP\n",
        "\n",
        "**Что нужно сделать:**\n",
        "- Для каждого изображения и вопроса создайте список возможных ответов (используйте answer_vocab)\n",
        "- Сформируйте промпты: \\\"Question: {question}. Answer: {answer}\\\"\n",
        "- Используйте CLIP для выбора наиболее подходящего ответа"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clip_predict"
      },
      "outputs": [],
      "source": [
        "def predict_clip(image, question, candidate_answers):\n",
        "    prompts = [f\"Question: {question}. Answer: {answer}\" for answer in candidate_answers]\n",
        "    \n",
        "    inputs = clip_processor(text=prompts, images=image, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "    \n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "    best_idx = probs.argmax().item()\n",
        "    \n",
        "    return candidate_answers[best_idx]\n",
        "\n",
        "candidate_answers = list(answer_vocab.keys())\n",
        "for i in range(min(3, len(df))):\n",
        "    row = df.iloc[i]\n",
        "    pred = predict_clip(sample_images[row['image_id']], row['question'], candidate_answers)\n",
        "    print(f\"\\nПример {i+1}:\")\n",
        "    print(f\"Question: {row['question']}\")\n",
        "    print(f\"True Answer: {row['answer']}\")\n",
        "    print(f\"CLIP Predicted: {pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llava"
      },
      "source": [
        "## 6. LLaVA Inference\n",
        "\n",
        "LLaVA (Large Language and Vision Assistant) — это большая мультимодальная модель, которая может генерировать текстовые ответы на вопросы об изображениях.\n",
        "\n",
        "**Внимание:** LLaVA-1.5-7B требует ~14GB GPU памяти. Если в Colab недостаточно памяти, используйте квантизацию (8-bit) или напишите мне про датасферу.\n",
        "\n",
        "### Задание 6.1: Загрузите LLaVA\n",
        "\n",
        "**Что нужно сделать:**\n",
        "- Загрузите модель `llava-hf/llava-1.5-7b-hf`\n",
        "- При необходимости используйте квантизацию для экономии памяти"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_llava"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "llava_model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    \"llava-hf/llava-1.5-7b-hf\",\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "llava_processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llava_inference"
      },
      "source": [
        "### Задание 6.2: Генерация ответов с LLaVA\n",
        "\n",
        "**Что нужно сделать:**\n",
        "- Реализуйте функцию для генерации ответов\n",
        "- Используйте формат промпта: \"USER: <image>\n",
        "Question: {question}\n",
        "ASSISTANT:\"\n",
        "- Протестируйте на 2-3 примерах"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llava_predict"
      },
      "outputs": [],
      "source": [
        "def predict_llava(image, question):\n",
        "    prompt = f\"USER: <image>\\nQuestion: {question}\\nASSISTANT:\"\n",
        "    \n",
        "    inputs = llava_processor(text=prompt, images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(llava_model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output = llava_model.generate(**inputs, max_new_tokens=50)\n",
        "    \n",
        "    answer = llava_processor.decode(output[0], skip_special_tokens=True)\n",
        "    answer = answer.split(\"ASSISTANT:\")[-1].strip()\n",
        "    \n",
        "    return answer\n",
        "\n",
        "for i in range(min(3, len(df))):\n",
        "    row = df.iloc[i]\n",
        "    pred = predict_llava(sample_images[row['image_id']], row['question'])\n",
        "    print(f\"\\nПример {i+1}:\")\n",
        "    print(f\"Question: {row['question']}\")\n",
        "    print(f\"True Answer: {row['answer']}\")\n",
        "    print(f\"LLaVA Predicted: {pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparison"
      },
      "source": [
        "## 7. Сравнение результатов\n",
        "\n",
        "Теперь сравним все три подхода на одних и тех же примерах.\n",
        "\n",
        "### Задание 7.1: Соберите результаты всех моделей\n",
        "\n",
        "Что нужно сделать:\n",
        "- Для каждого примера из датасета получите предсказания от всех трёх моделей\n",
        "- Создайте сравнительную таблицу\n",
        "- Проанализируйте, где какая модель работает лучше"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "collect_results"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    'Image ID': [],\n",
        "    'Question': [],\n",
        "    'True Answer': [],\n",
        "    'ResNet+T5': [],\n",
        "    'CLIP': [],\n",
        "    'LLaVA': []\n",
        "}\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    image = sample_images[row['image_id']]\n",
        "    question = row['question']\n",
        "    \n",
        "    results['Image ID'].append(row['image_id'])\n",
        "    results['Question'].append(question)\n",
        "    results['True Answer'].append(row['answer'])\n",
        "    results['ResNet+T5'].append(predict_baseline(row['image_id'], question))\n",
        "    results['CLIP'].append(predict_clip(image, question, candidate_answers))\n",
        "    results['LLaVA'].append(predict_llava(image, question))\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "analysis"
      },
      "source": [
        "### Задание 7.2: Проанализируйте результаты\n",
        "\n",
        "Что нужно сделать:\n",
        "- Посчитайте accuracy для каждой модели\n",
        "- Опишите сильные и слабые стороны каждого подхода\n",
        "- Приведите примеры, где модели ошибаются или дают разные ответы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calculate_metrics"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(predictions, true_answers):\n",
        "    correct = sum([1 for pred, true in zip(predictions, true_answers) if pred.lower() == true.lower()])\n",
        "    return correct / len(true_answers)\n",
        "\n",
        "baseline_acc = calculate_accuracy(results_df['ResNet+T5'], results_df['True Answer'])\n",
        "clip_acc = calculate_accuracy(results_df['CLIP'], results_df['True Answer'])\n",
        "llava_acc = calculate_accuracy(results_df['LLaVA'], results_df['True Answer'])\n",
        "\n",
        "print(\"\\nТочность моделей:\")\n",
        "print(f\"ResNet+T5: {baseline_acc:.2%}\")\n",
        "print(f\"CLIP: {clip_acc:.2%}\")\n",
        "print(f\"LLaVA: {llava_acc:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize_comparison"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "models = ['ResNet+T5', 'CLIP', 'LLaVA']\n",
        "accuracies = [baseline_acc, clip_acc, llava_acc]\n",
        "plt.bar(models, accuracies)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Comparison')\n",
        "plt.ylim([0, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "analysis_text"
      },
      "source": [
        "### Выводы (заполните после экспериментов):\n",
        "\n",
        "Baseline (ResNet + T5):\n",
        "- Сильные стороны: [Ваш ответ]\n",
        "- Слабые стороны: [Ваш ответ]\n",
        "\n",
        "CLIP:\n",
        "- Сильные стороны: [Ваш ответ]\n",
        "- Слабые стороны: [Ваш ответ]\n",
        "\n",
        "LLaVA:\n",
        "- Сильные стороны: [Ваш ответ]\n",
        "- Слабые стороны: [Ваш ответ]\n",
        "\n",
        "Общие наблюдения:\n",
        "[Ваши выводы о том, какие модели лучше справляются с разными типами вопросов]\n",
        "\n",
        "Мнения о домашке: [Ваш ответ]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}